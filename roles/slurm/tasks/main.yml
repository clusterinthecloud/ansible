---
- name: install munge
  yum:
    name: munge
    state: installed

- name: copy in munge key
  copy:
    src: munge.key
    dest: /etc/munge/munge.key
    owner: munge
    group: munge
    mode: 0400

- name: start service munge
  service:
    name: munge
    enabled: yes
    state: started

- name: install firewalld
  package:
    name: firewalld
    state: present

- name: install python-firewall
  package:
    name: python-firewall
    state: present

- name: start service firewalld
  service:
    name: firewalld
    enabled: yes
    state: started

- name: Add Slurm repository
  yum_repository:
    name: slurm
    description: Slurm
    baseurl: https://download.opensuse.org/repositories/home:/Milliams:/citc/CentOS%20EPEL/
    gpgcheck: yes
    gpgkey: https://download.opensuse.org/repositories/home:/Milliams:/citc/CentOS%20EPEL/repodata/repomd.xml.key

- name: create slurm group
  group:
    name: slurm
    state: present
    system: yes
    gid: 245

- name: create slurm user
  user:
    name: slurm
    comment: Slurm controller user
    group: slurm
    system: yes
    uid: 245

- name: install Slurm
  yum: name=slurm-{{ item }}-{{ slurm_version }}* state=present
  loop: "{{ slurm_packages }}"

- name: create slurm config directory
  file:
    path: /etc/slurm
    state: directory
    mode: 0755

- name: slurm config file
  template:
    src: slurm.conf.j2
    dest: /etc/slurm/slurm.conf
    owner: slurm
    group: slurm
    mode: 0644
  notify:
    - restart slurmd

- name: make home directory
  file:
    path: /mnt{{ filesystem_mount_point }}/etc/slurm
    state: directory
  when: slurm_role == "mgmt"

- name: slurm config file (shared)
  template:
    src: slurm_shared.conf.j2
    dest: /mnt/shared/etc/slurm/slurm.conf
    owner: slurm
    group: slurm
    mode: 0644
  when: slurm_role == "mgmt"
  notify:
    - restart slurmd

- name: slurmdbd config file
  template:
    src: slurmdbd.conf.j2
    dest: /etc/slurm/slurmdbd.conf
    owner: slurm
    group: slurm
    mode: 0400
  when: slurm_role == "mgmt"
  notify:
    - restart slurmdbd

- name: cgroup config file
  template:
    src: cgroup.conf.j2
    dest: /etc/slurm/cgroup.conf
    owner: slurm
    group: slurm
    mode: 0400
  notify:
    - restart slurmd

- name: set slurm log directory permissions
  file:
    path: /var/log/slurm/
    state: directory
    owner: slurm
    group: slurm
    mode: 0755

- name: set slurm spool directory permissions
  file:
    path: /var/spool/slurm/
    state: directory
    owner: slurm
    group: slurm
    mode: 0755
  when: slurm_role == "mgmt"

- name: set slurmd config directory permissions
  file:
    path: /var/spool/slurmd/
    state: directory
    owner: root
    group: root
    mode: 0755
  when: slurm_role == "compute"

- name: open all ports
  firewalld:
    zone: trusted
    source: 10.0.0.0/8
    immediate: true
    permanent: true
    state: enabled
  notify:
    - restart firewalld
    - restart slurmd

- include_tasks: elastic.yml
  when: slurm_role == "mgmt"

- include_tasks: elastic_oci.yml
  when: slurm_role == "mgmt" and ansible_local.citc.csp == "oracle"

- include_tasks: elastic_gcp.yml
  when: slurm_role == "mgmt" and ansible_local.citc.csp == "google"

- include_tasks: elastic_aws.yml
  when: slurm_role == "mgmt" and ansible_local.citc.csp == "aws"

- name: start service slurmctld
  service:
    name: slurmctld
    enabled: yes
  when: slurm_role == "mgmt"

- name: start service slurmdbd
  service:
    name: slurmdbd
    state: started
    enabled: yes
  when: slurm_role == "mgmt"

- name: start service slurmd
  service:
    name: slurmd
    state: started
    enabled: yes
  when: slurm_role == "compute"

# As of Slurm 18.08 the slurmdbd service doesn't become available immediately
# This ensures that the service is available before continuing
- name: Wait for slurmdbd to start
  wait_for:
    port: 6819
  when: slurm_role == "mgmt"

- name: create accounting cluster
  command: sacctmgr --immediate add cluster {{ slurm_cluster_name }}
  register: create_cluster_result
  changed_when: "create_cluster_result.rc == 0"
  failed_when: "create_cluster_result.rc != 0 and create_cluster_result.stdout != ' This cluster {{ slurm_cluster_name }} already exists.  Not adding.'"
  when: slurm_role == "mgmt"
