---
- name: install munge
  yum:
    name: munge
    state: installed

# If we're the mgmt node, create a munge key
- name: generate munge key
  shell:
    cmd: dd if=/dev/urandom bs=1 count=1024 > munge.key
    creates: munge.key
  args:
    chdir: /etc/munge/
  when: slurm_role == "mgmt"

- name: check presence of munge config file
  stat:
    path: /etc/munge/munge.key
  register: munge_config_file

# If we're a compute node, grab the file placed by Packer
- name: copy munge.key
  copy:
    src: /tmp/munge.key
    dest: /etc/munge/munge.key
    remote_src: yes
    mode: 0400
  when:
    - slurm_role == "compute"
    - not munge_config_file.stat.exists

# Make sure that the Packer-placed file is deleted
- name: delete /tmp/munge.key
  file:
    path: /tmp/munge.key
    state: absent

- name: set permissions on munge key
  file:
    path: /etc/munge/munge.key
    owner: munge
    group: munge
    mode: 0400

- name: start service munge
  service:
    name: munge
    enabled: yes
    state: started

- name: install firewalld
  package:
    name: firewalld
    state: present

- name: install python-firewall
  package:
    name: python3-firewall
    state: present

- name: start service firewalld
  service:
    name: firewalld
    enabled: yes
    state: started

- name: create slurm group
  group:
    name: slurm
    state: present
    system: yes
    gid: 245

- name: create slurm user
  user:
    name: slurm
    comment: Slurm controller user
    group: slurm
    system: yes
    uid: 245

- name: install Slurm
  yum:
    name: slurm-{{ item }}-{{ slurm_version }}*
    state: present
  loop: "{{ slurm_packages }}"

- name: create slurm accounting database
  mysql_db:
    name: slurmacct
    state: present
  when: slurm_role == "mgmt"

- name: create slurm MySQL user
  mysql_user:
    name: slurm
    password: "{{ slurm_accounting_db_password }}"
    priv: 'slurmacct.*:ALL'
    state: present
  when: slurm_role == "mgmt"

- name: create slurm config directory
  file:
    path: /etc/slurm
    state: directory
    mode: 0755

- name: slurm config file
  template:
    src: slurm.conf.j2
    dest: /etc/slurm/slurm.conf
    owner: slurm
    group: slurm
    mode: 0644

- name: make slurm config directory
  file:
    path: /mnt/{{ filesystem_mount_point }}/etc/slurm
    state: directory
    mode: u=rwx,g=rx,o=rx
  when: slurm_role == "mgmt"

- name: slurm config file (shared)
  template:
    src: slurm_shared.conf.j2
    dest: /mnt/shared/etc/slurm/slurm.conf
    owner: slurm
    group: slurm
    mode: 0644
  when: slurm_role == "mgmt"

- name: slurmdbd config file
  template:
    src: slurmdbd.conf.j2
    dest: /etc/slurm/slurmdbd.conf
    owner: slurm
    group: slurm
    mode: 0600
  when: slurm_role == "mgmt"
  notify:
    - restart slurmdbd

- name: cgroup config file
  template:
    src: cgroup.conf.j2
    dest: /etc/slurm/cgroup.conf
    owner: slurm
    group: slurm
    mode: 0400

- name: set slurm log directory permissions
  file:
    path: /var/log/slurm/
    state: directory
    owner: slurm
    group: slurm
    mode: 0755

- name: set slurm spool directory permissions
  file:
    path: /var/spool/slurm/
    state: directory
    owner: slurm
    group: slurm
    mode: 0755
  when: slurm_role == "mgmt"

- name: set slurmd config directory permissions
  file:
    path: /var/spool/slurmd/
    state: directory
    owner: root
    group: root
    mode: 0755
  when: slurm_role == "compute"

- name: install SELinux-policy
  package:
    name: selinux-policy

- name: disable SELinux
  selinux:
    state: permissive
    policy: targeted

# Requires selinux:
#   allow firewalld_t cloud_init_t:dbus send_msg;
- name: open all ports
  firewalld:
    zone: trusted
    source: 10.0.0.0/8
    immediate: true
    permanent: true
    state: enabled
  notify:
    - restart firewalld

- include_tasks: elastic.yml
  when: slurm_role == "mgmt"

- include_tasks: elastic_oci.yml
  when: slurm_role == "mgmt" and ansible_local.citc.csp == "oracle"

- include_tasks: elastic_gcp.yml
  when: slurm_role == "mgmt" and ansible_local.citc.csp == "google"

- include_tasks: elastic_aws.yml
  when: slurm_role == "mgmt" and ansible_local.citc.csp == "aws"

- name: start service slurmctld
  service:
    name: slurmctld
    enabled: yes
  when: slurm_role == "mgmt"

- name: start service slurmdbd
  service:
    name: slurmdbd
    state: started
    enabled: yes
  when: slurm_role == "mgmt"

- name: enable service slurmd
  service:
    name: slurmd
    enabled: yes
  when: slurm_role == "compute"

# As of Slurm 18.08 the slurmdbd service doesn't become available immediately
# This ensures that the service is available before continuing
- name: Wait for slurmdbd to start
  wait_for:
    port: 6819
  when: slurm_role == "mgmt"

- name: create accounting cluster
  command: sacctmgr --immediate add cluster {{ slurm_cluster_name }}
  register: create_cluster_result
  changed_when: "create_cluster_result.rc == 0"
  failed_when: "create_cluster_result.rc != 0 and create_cluster_result.stdout != ' This cluster {{ slurm_cluster_name }} already exists.  Not adding.'"
  when: slurm_role == "mgmt"
